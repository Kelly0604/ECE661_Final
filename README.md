# ECE661_Final

## Important Files Including:

- `Experiments` Folder: this folder contains the functions for building the 6 white-box attacks, black-box attacks (RayS) and transferability test. Functions are organized in their relative file to ensure more straightforward usage.
- `Experiments_inNotebook` Folder: this folder contains the same experiments on generating adversarial attacks, testing model's robust accuracy and attacks transferability. These experiments are done in 3 seperate notebooks mainly for each type of model. We started our experiments in these notebooks on Google Colab to utilize better computing resources. However, for your convenience in viewing the functions, please refer to the Experiments folder which contain similar information. 
  - `ResNetPytorch.py`, `DataManagerPytorch.py`, `AttackWrappersWhiteBoxP.py` in this folder are sourced from the paper  "On the Robustness of Vision Transformers to Adversarial Examples" Githubt repository: https://github.com/MetaMain/ViTRobust/tree/main
 
## How to use this repository
1. Clone the repository to your local machine. 
2. Choose which methods you would prefer: a). downloading the jupyter notebooks and running them on your google Colab b). Running `GenerateAttacks.py` and `TransferabilityTest.py` file which include the model evaluation and transferability tests. 
  
## Description
In recent years, attention-based architectures such as Vision Transformers (ViTs) have revolutionized computer vision tasks, achieving state-of-the-art performance in image classification. These models often outperform or match the capabilities of traditional Convolutional Neural Networks (CNNs), which have been the dominant approach for decades. However, while CNNs have been extensively studied for their vulnerabilities to adversarial attacks and the corresponding defenses, the robustness of ViTs under such attacks remains under-explored. This gap raises critical questions regarding their applicability in security-sensitive environments.

Adversarial attacks, which involve crafting small but malicious perturbations to input data, can cause neural networks to misclassify inputs with high confidence. Such attacks pose significant risks to real-world applications, including autonomous systems, healthcare diagnostics, and security systems. While CNNs have been extensively evaluated for adversarial robustness, ViTs, given their fundamentally different architectural principles, may exhibit distinct behaviors under similar attack scenarios.

This project seeks to bridge the gap in understanding the adversarial robustness of ViTs compared to CNNs. Specifically, we analyze the performance of ResNet56, ViT-B-32, and ensemble models (ViT-L-16 combined with BiT-M-R101x3) against various white-box and black-box adversarial attack methods. The study includes a comprehensive evaluation of transferability—how adversarial examples generated by one model can fool another—and aims to identify unique vulnerabilities and strengths across architectures.

Through exploring attack scenarios and assessing individual as well as ensemble defenses, this project adds valuable perspectives to the discussion on adversarial robustness in modern neural networks and offers practical guidance for developing resilient models.

## Hypothesis 
Null Hypothesis: There is no significant difference between the effect of generated adversarial examples across models (ViTs vs CNNs). 
Alternative Hypothesis: There is significant difference between the effect of generated adversarial examples across models (ViTs vs CNNs). 

## Dataset
CIFAR-10: In this project, we will use the CIFAR-10 dataset to evaluate and compare the robustness of various models, including Vision Transformers, CNNs, and ensemble architectures, against a range of adversarial attacks. This dataset, consisting of 60,000 32 x 32 color images across 10 classes, provides a well-established benchmark for testing model performance under adversarial conditions. 
